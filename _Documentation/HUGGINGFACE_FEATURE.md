# FonctionnalitÃ© HuggingFace GGUF

## Vue d'ensemble

Cette fonctionnalitÃ© permet de rechercher et tÃ©lÃ©charger des modÃ¨les GGUF directement depuis HuggingFace Hub dans l'application Ollama Manager.

## CaractÃ©ristiques

### ğŸ” Recherche AvancÃ©e

L'interface de recherche offre plusieurs filtres pour trouver le modÃ¨le parfait :

- **Recherche textuelle** : Recherchez par nom de modÃ¨le (llama, mistral, qwen, etc.)
- **Nombre de paramÃ¨tres** : Filtrez par taille de modÃ¨le (1B, 3B, 7B, 13B, 30B, 70B, 180B)
- **Quantification** : Choisissez le niveau de compression
  - Q2_K : TrÃ¨s petite (qualitÃ© rÃ©duite)
  - Q3_K_M : Petite (bon compromis)
  - Q4_K_M : Moyenne (recommandÃ©)
  - Q5_K_M : Grande (haute qualitÃ©)
  - Q6_K : TrÃ¨s grande
  - Q8_0 : Maximale (quasi-lossless)
- **Tri** : Par tÃ©lÃ©chargements, likes, date de mise Ã  jour, ou date de crÃ©ation
- **Limite de rÃ©sultats** : 10, 20, 50 ou 100 modÃ¨les

### ğŸ“¥ TÃ©lÃ©chargement Asynchrone

- TÃ©lÃ©chargement en arriÃ¨re-plan avec RQ (Redis Queue)
- Suivi de progression en temps rÃ©el via Server-Sent Events
- Barre de progression visuelle
- Statut de tÃ©lÃ©chargement en direct

### ğŸ“¦ Fichiers DÃ©taillÃ©s

Pour chaque modÃ¨le trouvÃ©, vous pouvez voir :
- Liste de tous les fichiers GGUF disponibles
- Taille de chaque fichier (Mo/Go)
- Niveau de quantification dÃ©tectÃ© automatiquement
- Nombre de paramÃ¨tres du modÃ¨le

## Architecture Technique

### Nouveaux Fichiers

```
app/
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ huggingface_client.py      # Client API HuggingFace
â”‚   â””â”€â”€ tasks.py                    # Workers ajoutÃ©s (enqueue_pull_gguf, pull_gguf_job)
â”œâ”€â”€ blueprints/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ routes_huggingface.py  # Routes API HuggingFace
â”‚   â””â”€â”€ core/
â”‚       â”œâ”€â”€ routes.py               # Route /huggingface ajoutÃ©e
â”‚       â””â”€â”€ templates/
â”‚           â””â”€â”€ huggingface.html    # Interface utilisateur
```

### Flux de DonnÃ©es

```
1. Utilisateur â†’ Formulaire de recherche
   â†“
2. GET /api/huggingface/search
   â†“
3. HuggingFaceClient.search_gguf_models()
   - Appel API HuggingFace
   - Filtrage des modÃ¨les GGUF
   - Parsing des mÃ©tadonnÃ©es
   â†“
4. Retour HTML (HTMX) avec liste des modÃ¨les
   â†“
5. Utilisateur clique "TÃ©lÃ©charger"
   â†“
6. POST /api/huggingface/pull
   â†“
7. enqueue_pull_gguf() â†’ RQ Job
   â†“
8. pull_gguf_job() en arriÃ¨re-plan
   - TÃ©lÃ©charge le fichier GGUF
   - Publie la progression sur Redis
   â†“
9. Frontend Ã©coute SSE /api/stream/progress
   - Met Ã  jour la barre de progression
   - Affiche le statut
```

## API Endpoints

### GET /api/huggingface/search

Recherche de modÃ¨les GGUF sur HuggingFace.

**ParamÃ¨tres de requÃªte :**
- `q` : Texte de recherche
- `limit` : Nombre de rÃ©sultats (dÃ©faut: 20)
- `sort` : Tri (downloads, likes, updated, created)
- `quantization` : Filtrer par quantification
- `parameter_size` : Filtrer par taille (ex: "7B")
- `min_downloads` : Nombre minimum de tÃ©lÃ©chargements

**RÃ©ponse :** HTML (HTMX) ou JSON

### POST /api/huggingface/pull

Lance le tÃ©lÃ©chargement d'un modÃ¨le GGUF.

**ParamÃ¨tres :**
- `model_id` : ID du modÃ¨le (ex: "TheBloke/Llama-2-7B-GGUF")
- `filename` : Nom du fichier GGUF
- `output_dir` : (Optionnel) RÃ©pertoire de sortie

**RÃ©ponse :** HTML avec barre de progression et script SSE

### GET /api/huggingface/model/<model_id>

RÃ©cupÃ¨re les dÃ©tails d'un modÃ¨le spÃ©cifique.

**RÃ©ponse :** JSON avec mÃ©tadonnÃ©es complÃ¨tes

### GET /api/huggingface/quantizations

Liste des niveaux de quantification disponibles.

**RÃ©ponse :** `{"quantizations": ["Q2_K", "Q3_K_M", ...]}`

### GET /api/huggingface/parameter_sizes

Liste des tailles de paramÃ¨tres communes.

**RÃ©ponse :** `{"parameter_sizes": ["1B", "3B", "7B", ...]}`

## Configuration

### Variables d'Environnement

Ajoutez Ã  votre fichier de configuration ou `.env` :

```bash
# Optionnel : Token HuggingFace pour accÃ©der aux modÃ¨les privÃ©s
HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxx

# Timeouts pour les requÃªtes HTTP (dÃ©jÃ  configurÃ©s)
HTTP_CONNECT_TIMEOUT=10
HTTP_READ_TIMEOUT=600
```

### RÃ©pertoire de TÃ©lÃ©chargement

Par dÃ©faut, les modÃ¨les GGUF sont tÃ©lÃ©chargÃ©s dans `/tmp/gguf_models/`.

Pour changer ce rÃ©pertoire, passez le paramÃ¨tre `output_dir` lors de l'appel API.

## Utilisation

### Interface Web

1. AccÃ©dez Ã  l'application Ollama Manager
2. Cliquez sur "HuggingFace" dans la navigation
3. Utilisez le formulaire de recherche :
   - Entrez un terme de recherche (ex: "llama")
   - (Optionnel) Ouvrez les filtres avancÃ©s
   - SÃ©lectionnez la taille de modÃ¨le souhaitÃ©e
   - Choisissez le niveau de quantification
   - Cliquez sur "Rechercher sur HuggingFace"
4. Parcourez les rÃ©sultats
5. Cliquez sur "Voir les fichiers" pour chaque modÃ¨le
6. Cliquez sur "TÃ©lÃ©charger" pour le fichier souhaitÃ©
7. Suivez la progression en temps rÃ©el

### API Directe

```bash
# Recherche
curl "http://localhost:5000/api/huggingface/search?q=llama&parameter_size=7B&quantization=Q4_K_M"

# TÃ©lÃ©chargement
curl -X POST http://localhost:5000/api/huggingface/pull \
  -d "model_id=TheBloke/Llama-2-7B-GGUF" \
  -d "filename=llama-2-7b.Q4_K_M.gguf"
```

## Parsing des MÃ©tadonnÃ©es

Le systÃ¨me analyse automatiquement les noms de fichiers GGUF pour extraire :

### Quantification

DÃ©tecte les patterns comme :
- `Q4_K_M` : Quantification 4-bit K-quant Medium
- `Q5_0` : Quantification 5-bit
- `F16` : Float16 (non quantifiÃ©)

Regex : `[._-](Q\d+_[KF]_[MSL]|Q\d+_\d+)[._-]`

### Taille de ParamÃ¨tres

DÃ©tecte les patterns comme :
- `7B` : 7 milliards de paramÃ¨tres
- `13B` : 13 milliards
- `70B` : 70 milliards

Regex : `[._-](\d+)B[._-]` ou `(\d+)b`

## DÃ©pendances

Les dÃ©pendances suivantes sont utilisÃ©es (dÃ©jÃ  prÃ©sentes) :

```
httpx>=0.24.0        # Client HTTP async
redis>=5.0.0         # Pub/Sub pour progression
rq>=1.15.0           # Job queue
flask>=3.0.0         # Framework web
```

## Exemples d'Utilisation

### Rechercher les meilleurs modÃ¨les Llama 7B quantifiÃ©s en Q4

```python
from app.services.huggingface_client import HuggingFaceClient

client = HuggingFaceClient()
models = client.search_gguf_models(
    query="llama",
    sort="downloads",
    filter_params={
        "parameter_size": "7B",
        "quantization": "Q4_K_M"
    }
)

for model in models:
    print(f"{model['id']} - {model['downloads']} tÃ©lÃ©chargements")
```

### TÃ©lÃ©charger un modÃ¨le spÃ©cifique

```python
from app.services.tasks import enqueue_pull_gguf

job_id = enqueue_pull_gguf(
    model_id="TheBloke/Llama-2-7B-GGUF",
    filename="llama-2-7b.Q4_K_M.gguf",
    output_dir="/path/to/models"
)

print(f"Job ID: {job_id}")
```

## Limitations Connues

1. **DÃ©pendance Redis** : NÃ©cessite Redis pour le suivi de progression
2. **Espace disque** : Les modÃ¨les GGUF peuvent Ãªtre volumineux (plusieurs Go)
3. **API HuggingFace** : Limite de taux potentielle (non authentifiÃ© : ~1000 req/h)
4. **Pas d'import automatique** : Les fichiers tÃ©lÃ©chargÃ©s doivent Ãªtre importÃ©s manuellement dans Ollama

## IntÃ©gration avec Ollama

Pour utiliser un modÃ¨le GGUF tÃ©lÃ©chargÃ© avec Ollama :

```bash
# CrÃ©er un Modelfile
cat > Modelfile << EOF
FROM /tmp/gguf_models/llama-2-7b.Q4_K_M.gguf
PARAMETER temperature 0.7
PARAMETER top_p 0.9
EOF

# CrÃ©er le modÃ¨le dans Ollama
ollama create my-llama-7b -f Modelfile
```

## AmÃ©liorations Futures

- [ ] Import automatique dans Ollama aprÃ¨s tÃ©lÃ©chargement
- [ ] Cache des rÃ©sultats de recherche
- [ ] Support des modÃ¨les privÃ©s avec authentification HF
- [ ] TÃ©lÃ©chargement parallÃ¨le de plusieurs fichiers
- [ ] PrÃ©visualisation des model cards
- [ ] Filtrage par licence (Apache, MIT, etc.)
- [ ] Support des modÃ¨les quantifiÃ©s GGML (anciens)
- [ ] Notifications push quand le tÃ©lÃ©chargement est terminÃ©

## Support et Contribution

Pour signaler des bugs ou proposer des amÃ©liorations :
- Ouvrez une issue sur GitHub
- Consultez la documentation HuggingFace : https://huggingface.co/docs

## Licence

Cette fonctionnalitÃ© fait partie d'Ollama Manager et est soumise Ã  la mÃªme licence que le projet principal.
