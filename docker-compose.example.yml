# Docker Compose Example - Nova
# 
# Copy this file to docker-compose.yml and adapt to your needs.
# Also configure the .env file (see .env.example)
#
# Usage:
#   docker compose up -d --build

services:
  # ===========================================
  # OLLAMA (optional - uncomment if you want Ollama in Docker)
  # ===========================================
  # If you prefer to run Ollama on the host, set OLLAMA_BASE_URL=http://host.docker.internal:11434 in .env
  
  # ollama:
  #   image: ollama/ollama:latest
  #   container_name: nova-ollama
  #   volumes:
  #     - ollama_data:/root/.ollama
  #   ports:
  #     - "11434:11434"
  #   restart: unless-stopped
  #   networks:
  #     - nova
  #   # GPU NVIDIA support (requires nvidia-container-toolkit)
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]

  # ===========================================
  # CORE SERVICES (required)
  # ===========================================

  redis:
    image: redis:7-alpine
    container_name: nova-redis
    volumes:
      - redis_data:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - nova

  postgres:
    image: pgvector/pgvector:pg16
    container_name: nova-postgres
    environment:
      - POSTGRES_USER=ollama
      - POSTGRES_PASSWORD=ollama_rag
      - POSTGRES_DB=ollama_rag
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ollama -d ollama_rag"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - nova

  # Qdrant - Base de données vectorielle pour RAG avancé
  qdrant:
    image: qdrant/qdrant:latest
    container_name: nova-qdrant
    volumes:
      - qdrant_data:/qdrant/storage
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:6333/readyz || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - nova

  # ===========================================
  # APPLICATION PRINCIPALE
  # ===========================================

  nova:
    build: .
    container_name: nova
    ports:
      - "5000:5000"
    environment:
      - FLASK_ENV=production
      - PYTHONUNBUFFERED=1
    env_file:
      - .env
    volumes:
      - ./app:/app/app
      - ./logs:/app/logs
      - gguf_models:/app/models
      - rag_uploads:/app/rag_uploads
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
    restart: unless-stopped
    command: gunicorn --bind 0.0.0.0:5000 --workers 2 --threads 4 --timeout 120 --access-logfile - --error-logfile - wsgi:app
    networks:
      - nova

  # Worker pour tâches asynchrones (téléchargements, indexation)
  nova-worker:
    build: .
    container_name: nova-worker
    environment:
      - PYTHONUNBUFFERED=1
    env_file:
      - .env
    volumes:
      - ./app:/app/app
      - ./logs:/app/logs
      - gguf_models:/app/models
      - rag_uploads:/app/rag_uploads
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
    restart: unless-stopped
    command: rq worker --url redis://redis:6379/0 ollama
    networks:
      - nova

  # ===========================================
  # SERVICES AUDIO (optionnels - nécessitent GPU NVIDIA)
  # ===========================================

  # Whisper - Speech to Text (décommentez si GPU disponible)
  # whisper:
  #   image: fedirz/faster-whisper-server:latest-cuda
  #   container_name: nova-whisper
  #   restart: unless-stopped
  #   environment:
  #     - WHISPER_MODEL=small
  #     - WHISPER_BEAM_SIZE=1
  #   volumes:
  #     - whisper_cache:/root/.cache/huggingface
  #   networks:
  #     - nova
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]

  # AllTalk - Text to Speech (décommentez si GPU disponible)
  # alltalk:
  #   image: erew123/alltalk_tts:latest
  #   container_name: nova-alltalk
  #   restart: unless-stopped
  #   environment:
  #     - DRIVER_AGREED=true
  #   volumes:
  #     - alltalk_models:/app/models
  #   ports:
  #     - "7851:7851"
  #   networks:
  #     - nova
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]

# ===========================================
# PERSISTENT VOLUMES
# ===========================================

volumes:
  redis_data:
  gguf_models:
  postgres_data:
  rag_uploads:
  qdrant_data:
  whisper_cache:
  alltalk_models:
  # ollama_data:  # Uncomment if using Ollama container

# ===========================================
# NETWORKS
# ===========================================

networks:
  nova:
    driver: bridge
  # Uncomment if using Traefik as reverse proxy
  # traefik_traefik:
  #   external: true
